# inference_engine/Cargo.toml
[package]
name = "inference_engine"
version = { workspace = true }
edition = { workspace = true }
authors = { workspace = true }
license = { workspace = true }
repository = { workspace = true }
publish = false
description = "Движок инференса для AI Агентов (LLM)."

[dependencies]
utils_crate = { path = "../utils_crate" }
model_loader = { path = "../model_loader" }
core_burn = { path = "../core_burn", default-features = false }
burn = { workspace = true }
tokenizers = { workspace = true } # ТОЧНО ТАК, КАК УНАСЛЕДОВАНО ОТ КОРНЕВОГО
tokio = { workspace = true }
tracing = { workspace = true }
serde = { workspace = true }
rand = { workspace = true }
rand_chacha = { workspace = true }
thiserror = { workspace = true }

burn-ndarray = { version = "0.17.0", default-features = false, features = ["std"], optional = true }
burn-tch = { version = "0.17.0", default-features = false, features = ["std"], optional = true }
burn-wgpu = { version = "0.17.0", default-features = false, features = ["std"], optional = true }
burn-autodiff = { version = "0.17.0", optional = true }

rayon = { workspace = true, optional = true } 

[dev-dependencies]
tempfile = { workspace = true }
serial_test = { workspace = true }
tracing-subscriber = { workspace = true }
serde_json = { workspace = true }
burn-ndarray = { version = "0.17.0", default-features = false, features = ["std"] } 

[features]
default = ["ndarray_backend"]

ndarray_backend = ["dep:burn-ndarray", "core_burn/ndarray_backend"]
tch_backend = ["dep:burn-tch", "core_burn/tch_backend"]
wgpu_backend = ["dep:burn-wgpu", "core_burn/wgpu_backend"]

autodiff_support = ["dep:burn-autodiff", "core_burn/autodiff_support"]
rayon_support = ["dep:rayon"]
```
